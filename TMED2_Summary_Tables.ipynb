{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3bba488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39b906dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mhughes/miniconda3/envs/spr_2021s_env/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# import plotting libraries\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn') # pretty matplotlib plots\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set('notebook', font_scale=1.25, style='whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc7bd25",
   "metadata": {},
   "source": [
    "## Setup Paths on your System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff97c94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_TMED2 = \"/Users/mhughes/Box/TMED2/approved_users_only/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bd855f",
   "metadata": {},
   "source": [
    "# Utility Functions\n",
    "\n",
    "Simple functions, reused later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e19d6791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_fine_diagnosis_str_to_int(s):\n",
    "    if s.count(\"no\"):\n",
    "        return 0\n",
    "    elif s.count(\"mild_AS\"):\n",
    "        return 1\n",
    "    elif s.count(\"mildtomod_AS\"):\n",
    "        return 2\n",
    "    elif s.count(\"moderate_AS\"):\n",
    "        return 3\n",
    "    elif s.count(\"severe_AS\"):\n",
    "        return 4\n",
    "    else:\n",
    "        raise ValueError(\"BAD STRING\")\n",
    "        \n",
    "def convert_int_to_diagnosis_str(i):\n",
    "    if i == 0:\n",
    "        return \"no_AS\"\n",
    "    elif i == 1:\n",
    "        return \"mild_AS\"\n",
    "    elif i == 2:\n",
    "        return \"mildtomod_AS\"\n",
    "    elif i == 3:\n",
    "        return \"moderate_AS\"\n",
    "    elif i == 4:\n",
    "        return \"severe_AS\"\n",
    "    else:\n",
    "        raise ValueError(\"BAD INT VALUE (not in 0-4)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68225265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_coarse_label(s):\n",
    "    if s.count(\"mild\"):\n",
    "        return 'early_AS'\n",
    "    elif s.count(\"no\"):\n",
    "        return 'no_AS'\n",
    "    else:\n",
    "        return 'significant_AS'\n",
    "    \n",
    "def make_coarse_label_column_from_df(s_arr):\n",
    "    return [make_coarse_label(s) for s in s_arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfa906b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_percentile_func(n):\n",
    "    def percentile_(x):\n",
    "        return np.percentile(x, n)\n",
    "    percentile_.__name__ = 'percentile_%s' % n\n",
    "    return percentile_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d8c03f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_description(df, colname, percentiles=[0.0,.05,.50,.95,1.00]):\n",
    "    descr_df = df[colname].describe(percentiles)\n",
    "    \n",
    "    b_df = pd.DataFrame([descr_df.values], columns=descr_df.index.values)\n",
    "    keys = ['count'] + ['%d%%' % (100*p) for p in percentiles]\n",
    "    print(b_df[keys].to_string(index=False))\n",
    "    return b_df, keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1923c440",
   "metadata": {},
   "source": [
    "## Function to load labeled set summary dataframe from CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87eed82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fold_specific_dataframes(\n",
    "        labeled_csv_relpath='DEV479/TMED2_fold0_labeledpart.csv',\n",
    "        unlabeled_csv_relpath=None,\n",
    "        count_key = '__count__',\n",
    "        verbose=False,\n",
    "        ):\n",
    "    ''' Load dataframes describing labels and train/test allocations for each patient/study/image\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    all_image_df : Dataframe with one row for every image (labeled and unlabeled)\n",
    "    labeled_image_df : Dataframe with one row per labeled image\n",
    "    study_df : Dataframe with one row per study\n",
    "    patient_df : Dataframe with one row per patient\n",
    "    '''\n",
    "    # Load CSV describing every view-labeled image fully-labeled set\n",
    "    labeled_df = pd.read_csv(os.path.join(PATH_TO_TMED2, labeled_csv_relpath))\n",
    "\n",
    "    # Load CSV describing unlabeled images that correspond to the studies in labeled set\n",
    "    if unlabeled_csv_relpath is None:\n",
    "        unlabeled_csv_relpath = labeled_csv_relpath.replace(\"_labeled\", \"_unlabeled\")\n",
    "    unlabeled_df = pd.read_csv(os.path.join(PATH_TO_TMED2, unlabeled_csv_relpath))\n",
    "    \n",
    "    # Convert patient_id and study_id to integers for easy operations\n",
    "    # Also create a patient_study_id key that uniquely ids each patient-study by concat-ing the two together\n",
    "    for name, df in [('labeled',labeled_df), ('unlabeled',unlabeled_df)]:\n",
    "        df[['patient_id_str','study_id_str','_unused_']] = df['query_key'].str.split(\"s|_\", expand=True)\n",
    "        df['patient_id'] = df['patient_id_str'].astype(int)\n",
    "        df['study_id'] = df['study_id_str'].astype(int)\n",
    "        assert df['patient_id'].max() < 10000\n",
    "        df['patient_study_id'] = df['patient_id'] * 10000 + df['study_id']\n",
    "        if verbose:\n",
    "            print(name.upper())\n",
    "            print(\"raw num patients: %5d\" % df['patient_id'].unique().size)\n",
    "            print(\"raw num studies : %5d\" % df['patient_study_id'].unique().size)\n",
    "    \n",
    "    # Keep only the images with a view label in the fully-labeled set\n",
    "    labeled_image_df = labeled_df.query(\"SourceFolder == 'view_and_diagnosis_labeled_set/labeled'\").copy()\n",
    "    labeled_image_df.sort_values(['patient_study_id', 'query_key', 'view_label'], inplace=True)\n",
    "\n",
    "    # Build dataframe where each row is a unique PATIENT-STUDY with 1+ images in fully-labeled set\n",
    "    study_keys = ['patient_id','study_id','patient_study_id',\n",
    "                  'view_classifier_split','diagnosis_classifier_split', 'diagnosis_label']\n",
    "    study_df = labeled_image_df[study_keys].drop_duplicates(keep='last')\n",
    "    study_df.sort_values(['patient_study_id'], inplace=True)\n",
    "\n",
    "    # Build dataframe where each row is a unique PATIENT with 1+ images in fully-labeled set\n",
    "    patient_keys = ['patient_id','view_classifier_split','diagnosis_classifier_split']\n",
    "    patient_df = labeled_image_df[patient_keys].drop_duplicates(keep='last')\n",
    "    patient_df.sort_values(['patient_id'], inplace=True)\n",
    "\n",
    "    # Verify each row of study_df is uniquely keyed by patient_study_id\n",
    "    assert study_df['patient_study_id'].unique().size == study_df.shape[0]\n",
    "\n",
    "    # Verify each row of patient_df is uniquely keyed by patient_id\n",
    "    assert patient_df['patient_id'].unique().size == patient_df.shape[0]\n",
    "    \n",
    "    # Verify view and diagnosis split assignments always the same\n",
    "    assert np.all(patient_df['view_classifier_split'] == patient_df['diagnosis_classifier_split'])\n",
    "    assert np.all(study_df['view_classifier_split'] == study_df['diagnosis_classifier_split'])\n",
    "    \n",
    "    # Verify each patient's studies always assigned to exactly one split\n",
    "    for p in patient_df['patient_id']:\n",
    "        q_df = study_df.query(\"patient_id == %d\" % p)\n",
    "        vals = q_df['view_classifier_split'].unique()\n",
    "        assert vals.size == 1\n",
    "        vals = q_df['diagnosis_classifier_split'].unique()\n",
    "        assert vals.size == 1\n",
    "\n",
    "    if verbose:\n",
    "        # Pretty print the first 10 and last 10 images from labeled set\n",
    "        def pretty_print_img_df(img_df):\n",
    "            simple_view_df = img_df[['query_key', 'view_label', 'diagnosis_label', 'diagnosis_classifier_split']]\n",
    "            print(simple_view_df.head(6).to_string(index=False))\n",
    "            print(\"...\")\n",
    "            print(simple_view_df.tail(6).to_string(index=False, header=False))\n",
    "        print(\"Loaded from filepath: %s\" % labeled_csv_relpath)\n",
    "        pretty_print_img_df(labeled_image_df)\n",
    "    \n",
    "    # Grab unlabeled images that correspond to the labeled studies\n",
    "    # Make sure these images are put into the right split\n",
    "    keep_df_list = list()\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        q_df = study_df.query(\"diagnosis_classifier_split == '%s'\" % split)\n",
    "        labeled_study_ids = q_df['patient_study_id'].unique()\n",
    "        q_unlabeled_img_df = unlabeled_df.loc[\n",
    "            unlabeled_df['patient_study_id'].isin(labeled_study_ids).values].copy()\n",
    "        q_unlabeled_img_df['diagnosis_classifier_split'] = split\n",
    "        keep_df_list.append(q_unlabeled_img_df)\n",
    "    keep_unlabeled_img_df = pd.concat(keep_df_list)\n",
    "\n",
    "    # Build combined dataframe of ALL images (labeled and unlabeled)\n",
    "    # FYI some \"query_key\" are redundant due to separate indexing in lab/unlab sets\n",
    "    all_image_df = pd.concat([labeled_image_df, keep_unlabeled_img_df])\n",
    "    all_image_df.sort_values(['patient_study_id', 'query_key', 'view_label'], inplace=True)\n",
    "\n",
    "    # Clean up all dataframes (fresh indexing from 0,1,... for each one)\n",
    "    all_image_df = all_image_df.reset_index(drop=True)\n",
    "    labeled_image_df = labeled_image_df.reset_index(drop=True)\n",
    "    study_df = study_df.reset_index(drop=True)\n",
    "    patient_df = patient_df.reset_index(drop=True)\n",
    "    \n",
    "    # Count number of images per study\n",
    "    all_image_df[count_key] = 1\n",
    "    labeled_image_df[count_key] = 1\n",
    "    study_df['n_all_image_per_study'] = all_image_df.groupby(['patient_study_id'], as_index=False).agg(\n",
    "        {count_key: 'sum'})[count_key]\n",
    "    study_df['n_labeled_image_per_study'] = labeled_image_df.groupby(['patient_study_id'], as_index=False).agg(\n",
    "        {count_key: 'sum'})[count_key]\n",
    "    study_df['diagnosis_label_as_int'] = [map_fine_diagnosis_str_to_int(s) for s in study_df['diagnosis_label'].values]\n",
    "    \n",
    "    # Count number of studies per patient and worst diagnosis per patient\n",
    "    study_df[count_key] = 1\n",
    "    patient_df['n_study_per_patient'] = study_df.groupby(['patient_id'], as_index=False).agg(\n",
    "        {count_key: 'sum'})[count_key]\n",
    "    patient_df['worst_diagnosis_label_as_int'] = study_df.groupby(['patient_id'], as_index=False).agg(\n",
    "        {'diagnosis_label_as_int': 'max'})['diagnosis_label_as_int']\n",
    "    patient_df['worst_diagnosis_label'] = [convert_int_to_diagnosis_str(i) for i in patient_df['worst_diagnosis_label_as_int']]\n",
    "\n",
    "    return all_image_df, labeled_image_df, study_df, patient_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fededc0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== FOLD fold0\n",
      "TOTAL PATIENTS\n",
      "576\n",
      "TOTAL STUDIES\n",
      "598\n",
      "STUDIES PER PATIENT\n",
      " 554 patients contributed 1 studies\n",
      "  22 patients contributed 2 studies\n",
      "TOTAL IMAGES\n",
      "43823\n",
      "TOTAL LABELED IMAGES\n",
      "17270\n",
      "NUM ALL IMAGES PER STUDY\n",
      " count   0%   5%  50%   95%  100%\n",
      " 598.0 15.0 48.0 70.0 105.1 181.0\n",
      "NUM LABELED IMAGES PER STUDY\n",
      " count  0%  5%  50%  95%  100%\n",
      " 598.0 1.0 4.9 19.0 71.0 107.0\n",
      "\n",
      "============== FOLD fold1\n",
      "TOTAL PATIENTS\n",
      "576\n",
      "TOTAL STUDIES\n",
      "598\n",
      "STUDIES PER PATIENT\n",
      " 554 patients contributed 1 studies\n",
      "  22 patients contributed 2 studies\n",
      "TOTAL IMAGES\n",
      "43823\n",
      "TOTAL LABELED IMAGES\n",
      "17270\n",
      "NUM ALL IMAGES PER STUDY\n",
      " count   0%   5%  50%   95%  100%\n",
      " 598.0 15.0 48.0 70.0 105.1 181.0\n",
      "NUM LABELED IMAGES PER STUDY\n",
      " count  0%  5%  50%  95%  100%\n",
      " 598.0 1.0 4.9 19.0 71.0 107.0\n",
      "\n",
      "============== FOLD fold2\n",
      "TOTAL PATIENTS\n",
      "576\n",
      "TOTAL STUDIES\n",
      "598\n",
      "STUDIES PER PATIENT\n",
      " 554 patients contributed 1 studies\n",
      "  22 patients contributed 2 studies\n",
      "TOTAL IMAGES\n",
      "43823\n",
      "TOTAL LABELED IMAGES\n",
      "17270\n",
      "NUM ALL IMAGES PER STUDY\n",
      " count   0%   5%  50%   95%  100%\n",
      " 598.0 15.0 48.0 70.0 105.1 181.0\n",
      "NUM LABELED IMAGES PER STUDY\n",
      " count  0%  5%  50%  95%  100%\n",
      " 598.0 1.0 4.9 19.0 71.0 107.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fold_id, split_csv in [\n",
    "        ('fold0', 'DEV479/TMED2_fold0_labeledpart.csv'),\n",
    "        ('fold1', 'DEV479/TMED2_fold1_labeledpart.csv'),\n",
    "        ('fold2', 'DEV479/TMED2_fold2_labeledpart.csv'),\n",
    "        ]:\n",
    "    \n",
    "    all_image_df, labeled_image_df, study_df, patient_df = load_fold_specific_dataframes(split_csv)    \n",
    "\n",
    "    print(\"============== FOLD %s\" % fold_id)\n",
    "    print(\"TOTAL PATIENTS\")\n",
    "    print(patient_df.shape[0])\n",
    "    print(\"TOTAL STUDIES\")\n",
    "    print(study_df.shape[0])\n",
    "    print(\"STUDIES PER PATIENT\")\n",
    "    vals, counts = np.unique(patient_df['n_study_per_patient'], return_counts=1)\n",
    "    for vv, cc in zip(vals, counts):\n",
    "        print(\" %3d patients contributed %d studies\" % (cc, vv))    \n",
    "    print(\"TOTAL IMAGES\")\n",
    "    print(all_image_df.shape[0])\n",
    "    print(\"TOTAL LABELED IMAGES\")\n",
    "    print(labeled_image_df.shape[0])\n",
    "    \n",
    "    with pd.option_context('display.float_format', '{:0.1f}'.format):\n",
    "        print(\"NUM ALL IMAGES PER STUDY\")\n",
    "        pretty_print_description(study_df, 'n_all_image_per_study')\n",
    "        \n",
    "        print(\"NUM LABELED IMAGES PER STUDY\")\n",
    "        pretty_print_description(study_df, 'n_labeled_image_per_study')\n",
    "        \n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc62394",
   "metadata": {},
   "source": [
    "# Create tables summarizing cohorts across train/test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87dcd4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_train_test_label_contents_of_fold(\n",
    "        all_image_df, labeled_image_df, study_df, patient_df,\n",
    "        percentiles=[10, 20, 50, 80, 90],\n",
    "        verbose=False,\n",
    "        count_key = '__count__'\n",
    "        ):\n",
    "    row_dict_list = list()\n",
    "    for diagnosis_label in ['no_AS', 'mild_AS', 'mildtomod_AS', 'moderate_AS', 'severe_AS']:\n",
    "        for split in ['train', 'val', 'test']:\n",
    "            q_df = study_df.query(\n",
    "                \"diagnosis_label == '%s' and diagnosis_classifier_split == '%s'\" % (diagnosis_label, split))\n",
    "\n",
    "            row_dict = dict(\n",
    "                fine_label=diagnosis_label,\n",
    "                split=split,\n",
    "                num_patients=q_df['patient_id'].unique().size,\n",
    "                num_studies=q_df.shape[0],\n",
    "            )\n",
    "            # TODO add this key to summarize all images not just labeled ('all', all_image_df)\n",
    "            for imgtype_key, img_df in [\n",
    "                    ('all', all_image_df),\n",
    "                    ('labeled', labeled_image_df),\n",
    "                    ('frac_labeled', labeled_image_df)]:\n",
    "                iq_df = img_df.query(\n",
    "                    \"diagnosis_label == '%s' and diagnosis_classifier_split == '%s'\" % (\n",
    "                        diagnosis_label, split)).copy()\n",
    "                iq_df[count_key] = 1\n",
    "                n_im_per_study_df = iq_df.groupby(['patient_study_id'], as_index=False).agg(\n",
    "                    {count_key: 'sum'})[['patient_study_id', count_key]]\n",
    "                n_im_per_study = n_im_per_study_df[count_key]\n",
    "                if imgtype_key.startswith(\"frac\"):\n",
    "                    n_im_per_study /= n_im_per_study_all\n",
    "                \n",
    "                if verbose and split == 'train' and diagnosis_label == 'no_AS':\n",
    "                    print(\"Num %s images in first 10 studies in TRAIN with label no_AS\" % imgtype_key)\n",
    "                    print(n_im_per_study.values[:10])\n",
    "\n",
    "                if not imgtype_key.startswith('frac'):\n",
    "                    row_dict['num_images_%s' % imgtype_key] = iq_df.shape[0]\n",
    "                    \n",
    "                for perctile in percentiles:\n",
    "                    if imgtype_key.startswith('frac'):\n",
    "                        key = 'frac_images_per_study_%s_%02d' % (imgtype_key[5:], perctile)\n",
    "                    else:\n",
    "                        key = 'num_images_per_study_%s_%02d' % (imgtype_key, perctile)\n",
    "                    row_dict[key] = \\\n",
    "                        np.percentile(n_im_per_study, perctile)\n",
    "                if imgtype_key == 'all':\n",
    "                    n_im_per_study_all = np.asarray(n_im_per_study, dtype=np.float64)\n",
    "                \n",
    "            row_dict_list.append(row_dict)\n",
    "    \n",
    "    summary_df = pd.DataFrame(row_dict_list)\n",
    "    summary_df['coarse_label'] = make_coarse_label_column_from_df(summary_df['fine_label'])\n",
    "    summary_df['coarse_label'] = pd.Categorical(summary_df['coarse_label'], categories=['no_AS', 'early_AS', 'significant_AS'])\n",
    "    summary_df['split'] = pd.Categorical(summary_df['split'], categories=['train', 'val', 'test'])\n",
    "    \n",
    "    keys = [\n",
    "        'num_patients', 'num_studies',\n",
    "        'num_images_labeled'] + ['num_images_per_study_labeled_%02d' % p for p in percentiles]\n",
    "    if 'num_images_all' in row_dict:\n",
    "        keys = keys + [k.replace(\"_labeled\", \"_all\") for k in keys if k.count('_labeled')]\n",
    "        keys = keys + [k.replace(\"num_images_per_study_\", \"frac_images_per_study_\") for k in keys if k.count('num_images_per_study_labeled')]\n",
    "    keys = np.unique(keys).tolist()\n",
    "    coarse_df = summary_df[['coarse_label', 'split'] + keys].copy()\n",
    "    sum_df_by_key = dict()\n",
    "    for key in keys:\n",
    "        if key.count(\"frac_images\"):\n",
    "            aggfunc = 'max'\n",
    "            margins = False\n",
    "        elif key.count(\"per_study\"):\n",
    "            aggfunc = 'sum'\n",
    "            margins = False # cant aggregate acros per-study stats easily            \n",
    "        else:\n",
    "            aggfunc = 'sum'\n",
    "            margins = True\n",
    "        my_df = coarse_df.groupby(['coarse_label', 'split'], as_index=False).agg({key: aggfunc})\n",
    "        sum_df = pd.crosstab(my_df['split'], my_df['coarse_label'], my_df[key], aggfunc=aggfunc, margins=margins)\n",
    "        sum_df_by_key[key] = sum_df\n",
    "        if verbose:\n",
    "            print(\"=== KEY \" + key)\n",
    "            with pd.option_context('display.float_format', '{:0.2f}'.format):\n",
    "                print(sum_df)\n",
    "            print(\"\")\n",
    "    return sum_df_by_key #, n_im_per_study_all, n_im_per_study_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6779fc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pprint_tables_horizontal_stack(df, col1, col2):\n",
    "    str1 = df[col1].to_string(index=True).replace(\"coarse_label\", \"            \")\n",
    "    str2 = df[col2].to_string(index=True).replace(\"coarse_label\", \"            \")\n",
    "    L1, L2 = str1.count(\"\\n\"), str2.count(\"\\n\")\n",
    "    str2 += \"\\n\" * (L1-L2)\n",
    "    print(\"%50s  ||  %s\" % (col1.upper(), col2.upper()))\n",
    "    print(\"%50s  ||  %s\" % ('-' * 50, '-' * len(col2)))\n",
    "    for line1, line2 in zip(str1.split(\"\\n\"), str2.split(\"\\n\")):\n",
    "        print(\"%40s  ||  %s\" % (line1,line2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7351628",
   "metadata": {},
   "source": [
    "## FOLD 0 summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50fc7145",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold0_df = summarize_train_test_label_contents_of_fold(\n",
    "    *load_fold_specific_dataframes('DEV479/TMED2_fold0_labeledpart.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2703dcc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       NUM_STUDIES  ||  NUM_IMAGES_PER_STUDY_LABELED_50\n",
      "--------------------------------------------------  ||  -------------------------------\n",
      "              no_AS  early_AS  significant_AS  All  ||                no_AS  early_AS  significant_AS\n",
      "split                                               ||  split                                        \n",
      "train            76       103             181  360  ||  train          13.0      21.0            95.5\n",
      "val              25        34              60  119  ||  val            14.0      28.0           102.0\n",
      "test             25        34              60  119  ||  test           14.0      35.0           114.5\n",
      "All             126       171             301  598  ||  \n"
     ]
    }
   ],
   "source": [
    "pprint_tables_horizontal_stack(fold0_df, 'num_studies', 'num_images_per_study_labeled_50')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce764b85",
   "metadata": {},
   "source": [
    "## FOLD 1 summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b4ba100",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold1_df = summarize_train_test_label_contents_of_fold(\n",
    "    *load_fold_specific_dataframes('DEV479/TMED2_fold1_labeledpart.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a1d7a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       NUM_STUDIES  ||  NUM_IMAGES_PER_STUDY_LABELED_50\n",
      "--------------------------------------------------  ||  -------------------------------\n",
      "              no_AS  early_AS  significant_AS  All  ||                no_AS  early_AS  significant_AS\n",
      "split                                               ||  split                                        \n",
      "train            76       103             181  360  ||  train          13.0      22.0           100.5\n",
      "val              25        34              60  119  ||  val            14.0      23.0            97.0\n",
      "test             25        34              60  119  ||  test           10.0      28.0           110.0\n",
      "All             126       171             301  598  ||  \n"
     ]
    }
   ],
   "source": [
    "pprint_tables_horizontal_stack(fold1_df, 'num_studies', 'num_images_per_study_labeled_50')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001db674",
   "metadata": {},
   "source": [
    "## FOLD 2 summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a740ffa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       NUM_STUDIES  ||  NUM_IMAGES_PER_STUDY_LABELED_50\n",
      "--------------------------------------------------  ||  -------------------------------\n",
      "              no_AS  early_AS  significant_AS  All  ||                no_AS  early_AS  significant_AS\n",
      "split                                               ||  split                                        \n",
      "train            76       103             181  360  ||  train          13.5      22.0            99.5\n",
      "val              24        34              60  118  ||  val            13.0      22.0            95.5\n",
      "test             26        34              60  120  ||  test           13.5      31.0            98.0\n",
      "All             126       171             301  598  ||  \n"
     ]
    }
   ],
   "source": [
    "fold2_df = summarize_train_test_label_contents_of_fold(\n",
    "    *load_fold_specific_dataframes('DEV479/TMED2_fold2_labeledpart.csv'))\n",
    "\n",
    "pprint_tables_horizontal_stack(fold2_df, 'num_studies', 'num_images_per_study_labeled_50')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
